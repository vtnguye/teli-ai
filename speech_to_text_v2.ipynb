{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U openai-whisper\n",
    "#%pip install pyannote.audio\n",
    "#%pip install openai\n",
    "\n",
    "#### Usually only works in terminal #### FOR GPU ####\n",
    "#pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import subprocess\n",
    "import subprocess\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs GPU if device has one\\n\n",
    "import torch\n",
    "#Check if CUDA-enabled GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert MP3 into WAV\n",
    "Run the first two lines if you are using a virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg_path = \"C:\\\\PATH_Programs\\\\\"\n",
    "os.environ[\"PATH\"] += os.pathsep + ffmpeg_path\n",
    "\n",
    "def mp3_to_wav(folder_path):\n",
    "    # Iterate over all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file is an MP3\n",
    "        if file_name.endswith(\".mp3\"):\n",
    "            # Set the paths for the MP3 and WAV files\n",
    "            mp3_path = os.path.join(folder_path, file_name)\n",
    "            wav_path = os.path.join(folder_path, file_name[:-4] + \".wav\")\n",
    "            \n",
    "            # Use subprocess to run the ffmpeg command to convert the MP3 to WAV\n",
    "            subprocess.run([\"ffmpeg\", \"-i\", mp3_path, \"-ar\", \"16000\", wav_path], check=True)\n",
    "            \n",
    "            # Delete the original MP3 file\n",
    "            os.remove(mp3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Whisper and Pyannote Audio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_speakers = 2 #@param {type:\"integer\"}\n",
    "language = 'English' #@param ['any', 'English']\n",
    "model_size = \"large-v2\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large_v2\"]\n",
    "\n",
    "model_name = model_size\n",
    "if language == 'English' and model_size != 'medium':\n",
    "  model_name += '.en'\n",
    "\n",
    "model = whisper.load_model(model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "#Making sure the GPU is running\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(path, num_speakers=2,model=model):\n",
    "  result = model.transcribe(path)\n",
    "  output = result[\"text\"]\n",
    "  return output\n",
    "\n",
    "def write_file(text, file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(text)\n",
    "        \n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "def process_transcript(transcript):\n",
    "    prompt = f\"Given the following transcript of a telemarketing call between two speakers, organize the utterances by alternating speakers, without the prefix Speaker 1 or Speaker 2:\\n{transcript}\\n\\nUtterances:\\n\" \n",
    "    response = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=1024,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.8,\n",
    ")\n",
    "    processed_transcript = response.choices[0].text.strip()\n",
    "    return processed_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ffmpeg' has no attribute 'Error'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Austin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\whisper\\audio.py:46\u001b[0m, in \u001b[0;36mload_audio\u001b[1;34m(file, sr)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     \u001b[39m# This launches a subprocess to decode audio while down-mixing and resampling as necessary.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[39m# Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     out, _ \u001b[39m=\u001b[39m (\n\u001b[1;32m---> 46\u001b[0m         ffmpeg\u001b[39m.\u001b[39;49minput(file, threads\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     47\u001b[0m         \u001b[39m.\u001b[39moutput(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ms16le\u001b[39m\u001b[39m\"\u001b[39m, acodec\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpcm_s16le\u001b[39m\u001b[39m\"\u001b[39m, ac\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, ar\u001b[39m=\u001b[39msr)\n\u001b[0;32m     48\u001b[0m         \u001b[39m.\u001b[39mrun(cmd\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mffmpeg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-nostdin\u001b[39m\u001b[39m\"\u001b[39m], capture_stdout\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, capture_stderr\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m     )\n\u001b[0;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m ffmpeg\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ffmpeg' has no attribute 'input'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m file_result_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(result_path, file_name[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(file_result_path):\n\u001b[1;32m---> 10\u001b[0m     output \u001b[39m=\u001b[39mtranscribe(file_path)\n\u001b[0;32m     11\u001b[0m     output \u001b[39m=\u001b[39m process_transcript(output)\n\u001b[0;32m     12\u001b[0m     write_file(output, file_result_path)\n",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m, in \u001b[0;36mtranscribe\u001b[1;34m(path, num_speakers, model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranscribe\u001b[39m(path, num_speakers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,model\u001b[39m=\u001b[39mmodel):\n\u001b[1;32m----> 2\u001b[0m   result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtranscribe(path)\n\u001b[0;32m      3\u001b[0m   output \u001b[39m=\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m   \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Austin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\whisper\\transcribe.py:121\u001b[0m, in \u001b[0;36mtranscribe\u001b[1;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, **decode_options)\u001b[0m\n\u001b[0;32m    118\u001b[0m     decode_options[\u001b[39m\"\u001b[39m\u001b[39mfp16\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39m# Pad 30-seconds of silence to the input audio, for slicing\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m mel \u001b[39m=\u001b[39m log_mel_spectrogram(audio, padding\u001b[39m=\u001b[39;49mN_SAMPLES)\n\u001b[0;32m    122\u001b[0m content_frames \u001b[39m=\u001b[39m mel\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m N_FRAMES\n\u001b[0;32m    124\u001b[0m \u001b[39mif\u001b[39;00m decode_options\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlanguage\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Austin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\whisper\\audio.py:130\u001b[0m, in \u001b[0;36mlog_mel_spectrogram\u001b[1;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(audio):\n\u001b[0;32m    129\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(audio, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 130\u001b[0m         audio \u001b[39m=\u001b[39m load_audio(audio)\n\u001b[0;32m    131\u001b[0m     audio \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(audio)\n\u001b[0;32m    133\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Austin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\whisper\\audio.py:50\u001b[0m, in \u001b[0;36mload_audio\u001b[1;34m(file, sr)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     \u001b[39m# This launches a subprocess to decode audio while down-mixing and resampling as necessary.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[39m# Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     out, _ \u001b[39m=\u001b[39m (\n\u001b[0;32m     46\u001b[0m         ffmpeg\u001b[39m.\u001b[39minput(file, threads\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     47\u001b[0m         \u001b[39m.\u001b[39moutput(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ms16le\u001b[39m\u001b[39m\"\u001b[39m, acodec\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpcm_s16le\u001b[39m\u001b[39m\"\u001b[39m, ac\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, ar\u001b[39m=\u001b[39msr)\n\u001b[0;32m     48\u001b[0m         \u001b[39m.\u001b[39mrun(cmd\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mffmpeg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-nostdin\u001b[39m\u001b[39m\"\u001b[39m], capture_stdout\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, capture_stderr\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m     )\n\u001b[1;32m---> 50\u001b[0m \u001b[39mexcept\u001b[39;00m ffmpeg\u001b[39m.\u001b[39;49mError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     51\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to load audio: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mdecode()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mfrombuffer(out, np\u001b[39m.\u001b[39mint16)\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32) \u001b[39m/\u001b[39m \u001b[39m32768.0\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ffmpeg' has no attribute 'Error'"
     ]
    }
   ],
   "source": [
    "classifications = ['/not_interested']\n",
    "for classification in classifications:\n",
    "    audio_path = './asset/testing_audio' + classification\n",
    "    result_path = './asset/testing_result' + classification\n",
    "    mp3_to_wav(audio_path)\n",
    "    for file_name in os.listdir(audio_path):\n",
    "        file_path = os.path.join(audio_path, file_name)\n",
    "        file_result_path = os.path.join(result_path, file_name[:-4] + '.txt')\n",
    "        if not os.path.exists(file_result_path):\n",
    "            output =transcribe(file_path)\n",
    "            output = process_transcript(output)\n",
    "            write_file(output, file_result_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
