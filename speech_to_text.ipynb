{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yf8Oow4hgUU"
      },
      "source": [
        "# Install and Import Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr-qMvukhgUX"
      },
      "outputs": [],
      "source": [
        "%pip install -U openai-whisper\n",
        "%pip install pyannote.audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XIcBe27YhgUZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
            "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "import datetime\n",
        "\n",
        "import subprocess\n",
        "\n",
        "import torch\n",
        "import pyannote.audio\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8sVhspzf8tc"
      },
      "source": [
        "# Util Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J2cRhvwgQ6t"
      },
      "source": [
        "## Convert MP3 into WAV\n",
        "Run the first two lines if you are using a virtual environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6eJFVcyKhgUa"
      },
      "outputs": [],
      "source": [
        "#ffmpeg_path = \"C:\\\\ProgramData\\\\ffmpeg\"\n",
        "#os.environ[\"PATH\"] += os.pathsep + ffmpeg_path\n",
        "\n",
        "def mp3_to_wav(folder_path):\n",
        "    # Iterate over all files in the folder\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        # Check if the file is an MP3\n",
        "        if file_name.endswith(\".mp3\"):\n",
        "            # Set the paths for the MP3 and WAV files\n",
        "            mp3_path = os.path.join(folder_path, file_name)\n",
        "            wav_path = os.path.join(folder_path, file_name[:-4] + \".wav\")\n",
        "            \n",
        "            # Use subprocess to run the ffmpeg command to convert the MP3 to WAV\n",
        "            subprocess.run([\"ffmpeg\", \"-i\", mp3_path, \"-ar\", \"16000\", wav_path], check=True)\n",
        "            \n",
        "            # Delete the original MP3 file\n",
        "            os.remove(mp3_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Whisper and Pyannote Audio model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_speakers = 2 #@param {type:\"integer\"}\n",
        "language = 'English' #@param ['any', 'English']\n",
        "model_size = \"large-v2\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large_v2\"]\n",
        "\n",
        "model_name = model_size\n",
        "if language == 'English' and model_size != 'medium':\n",
        "  model_name += '.en'\n",
        "\n",
        "model = whisper.load_model(model_size)\n",
        "model_medium = whisper.load_model('medium') #Base model is used to test other function\n",
        "embedding_model = PretrainedSpeakerEmbedding( \n",
        "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Other functions to transcribe text with Speech Diarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FadBSwdRhgUc"
      },
      "outputs": [],
      "source": [
        "def get_duration(path):\n",
        "  with contextlib.closing(wave.open(path,'r')) as f:\n",
        "    frames = f.getnframes()\n",
        "    rate = f.getframerate()\n",
        "    return frames / float(rate)\n",
        "  \n",
        "def make_embeddings(path, segments, duration):\n",
        "  embeddings = np.zeros(shape=(len(segments), 192))\n",
        "  for i, segment in enumerate(segments):\n",
        "    embeddings[i] = segment_embedding(path, segment, duration)\n",
        "  return np.nan_to_num(embeddings)\n",
        "\n",
        "audio = Audio()\n",
        "\n",
        "def segment_embedding(path, segment, duration):\n",
        "  start = segment[\"start\"]\n",
        "  # Whisper overshoots the end timestamp in the last segment\n",
        "  end = min(duration, segment[\"end\"])\n",
        "  clip = Segment(start, end)\n",
        "  waveform, sample_rate = audio.crop(path, clip)\n",
        "  return embedding_model(waveform[None])\n",
        "\n",
        "def add_speaker_labels(segments, embeddings, num_speakers):\n",
        "  clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
        "  labels = clustering.labels_\n",
        "  for i in range(len(segments)):\n",
        "    segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
        "\n",
        "def time(secs):\n",
        "  return datetime.timedelta(seconds=round(secs))\n",
        "\n",
        "def get_output(segments):\n",
        "  output = ''\n",
        "  for (i, segment) in enumerate(segments):\n",
        "    output += segment[\"text\"][1:] + '\\n'\n",
        "  output += '\\n'\n",
        "  return output\n",
        "def write_file(text, file_path):\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(text)\n",
        "        \n",
        "def transcribe(path, num_speakers,model=model):\n",
        "  duration = get_duration(path)\n",
        "  if duration > 4 * 60 * 60:\n",
        "    return \"Audio duration too long\"\n",
        "\n",
        "  result = model.transcribe(path)\n",
        "  segments = result[\"segments\"]\n",
        "\n",
        "  num_speakers = min(max(round(num_speakers), 1), len(segments))\n",
        "  if len(segments) == 1:\n",
        "    segments[0]['speaker'] = 'SPEAKER 1'\n",
        "  else:\n",
        "    embeddings = make_embeddings(path, segments, duration)\n",
        "    add_speaker_labels(segments, embeddings, num_speakers)\n",
        "  output = get_output(segments)\n",
        "  return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP3AswjShE5O"
      },
      "source": [
        "# Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lLzz_J2xhgUe"
      },
      "outputs": [],
      "source": [
        "classification = '/successful'#Change this for different type of classification\n",
        "audio_path = './asset/testing_audio' + classification\n",
        "result_path = './asset/testing_result' + classification\n",
        "\n",
        "\n",
        "mp3_to_wav(audio_path)\n",
        "output=''\n",
        "for file_name in os.listdir(audio_path):\n",
        "    if os.path.isfile(os.path.join(audio_path, file_name)):\n",
        "        file_path = os.path.join(audio_path, file_name)\n",
        "        file_result_path = os.path.join(result_path, file_name[:-4] + '.txt')\n",
        "        output =transcribe(file_path, num_speakers,model=model_medium)\n",
        "        write_file(output, file_result_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8yf8Oow4hgUU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
