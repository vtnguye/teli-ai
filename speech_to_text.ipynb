{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yf8Oow4hgUU"
      },
      "source": [
        "# Install and Import Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr-qMvukhgUX"
      },
      "outputs": [],
      "source": [
        "%pip install -U openai-whisper\n",
        "%pip install pyannote.audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XIcBe27YhgUZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
            "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "import datetime\n",
        "\n",
        "import subprocess\n",
        "\n",
        "import torch\n",
        "import pyannote.audio\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8sVhspzf8tc"
      },
      "source": [
        "# Util Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J2cRhvwgQ6t"
      },
      "source": [
        "## Convert MP3 into WAV\n",
        "Run the first two lines if you are using a virtual environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6eJFVcyKhgUa"
      },
      "outputs": [],
      "source": [
        "#ffmpeg_path = \"C:\\\\ProgramData\\\\ffmpeg\"\n",
        "#os.environ[\"PATH\"] += os.pathsep + ffmpeg_path\n",
        "\n",
        "def mp3_to_wav(folder_path):\n",
        "    # Iterate over all files in the folder\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        # Check if the file is an MP3\n",
        "        if file_name.endswith(\".mp3\"):\n",
        "            # Set the paths for the MP3 and WAV files\n",
        "            mp3_path = os.path.join(folder_path, file_name)\n",
        "            wav_path = os.path.join(folder_path, file_name[:-4] + \".wav\")\n",
        "            \n",
        "            # Use subprocess to run the ffmpeg command to convert the MP3 to WAV\n",
        "            subprocess.run([\"ffmpeg\", \"-i\", mp3_path, \"-ar\", \"16000\", wav_path], check=True)\n",
        "            \n",
        "            # Delete the original MP3 file\n",
        "            os.remove(mp3_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Whisper and Pyannote Audio model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_speakers = 2 #@param {type:\"integer\"}\n",
        "language = 'English' #@param ['any', 'English']\n",
        "model_size = \"tiny\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large_v2\"]\n",
        "\n",
        "model_name = model_size\n",
        "if language == 'English' and model_size != 'medium':\n",
        "  model_name += '.en'\n",
        "\n",
        "model = whisper.load_model(model_size)\n",
        "model_medium = whisper.load_model('medium') #Base model is used to test other function\n",
        "embedding_model = PretrainedSpeakerEmbedding( \n",
        "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Other functions to transcribe text with Speech Diarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FadBSwdRhgUc"
      },
      "outputs": [],
      "source": [
        "def get_duration(path):\n",
        "  with contextlib.closing(wave.open(path,'r')) as f:\n",
        "    frames = f.getnframes()\n",
        "    rate = f.getframerate()\n",
        "    return frames / float(rate)\n",
        "  \n",
        "def make_embeddings(path, segments, duration):\n",
        "  embeddings = np.zeros(shape=(len(segments), 192))\n",
        "  for i, segment in enumerate(segments):\n",
        "    embeddings[i] = segment_embedding(path, segment, duration)\n",
        "  return np.nan_to_num(embeddings)\n",
        "\n",
        "audio = Audio()\n",
        "\n",
        "def segment_embedding(path, segment, duration):\n",
        "  start = segment[\"start\"]\n",
        "  # Whisper overshoots the end timestamp in the last segment\n",
        "  end = min(duration, segment[\"end\"])\n",
        "  clip = Segment(start, end)\n",
        "  waveform, sample_rate = audio.crop(path, clip)\n",
        "  return embedding_model(waveform[None])\n",
        "\n",
        "def add_speaker_labels(segments, embeddings, num_speakers):\n",
        "  clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
        "  labels = clustering.labels_\n",
        "  for i in range(len(segments)):\n",
        "    segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
        "\n",
        "def time(secs):\n",
        "  return datetime.timedelta(seconds=round(secs))\n",
        "\n",
        "def get_output(segments):\n",
        "  output = ''\n",
        "  for (i, segment) in enumerate(segments):\n",
        "    output += segment[\"text\"][1:] + '\\n'\n",
        "  output += '\\n'\n",
        "  return output\n",
        "def write_file(text, file_path):\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(text)\n",
        "        \n",
        "def transcribe(path, num_speakers,model=model):\n",
        "  duration = get_duration(path)\n",
        "  if duration > 4 * 60 * 60:\n",
        "    return \"Audio duration too long\"\n",
        "\n",
        "  result = model.transcribe(path)\n",
        "  segments = result[\"segments\"]\n",
        "\n",
        "  num_speakers = min(max(round(num_speakers), 1), len(segments))\n",
        "  if len(segments) == 1:\n",
        "    segments[0]['speaker'] = 'SPEAKER 1'\n",
        "  else:\n",
        "    embeddings = make_embeddings(path, segments, duration)\n",
        "    add_speaker_labels(segments, embeddings, num_speakers)\n",
        "  output = get_output(segments)\n",
        "  return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP3AswjShE5O"
      },
      "source": [
        "# Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lLzz_J2xhgUe"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(audio_path, file_name)\n\u001b[0;32m     11\u001b[0m file_result_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(result_path, file_name[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m output \u001b[39m=\u001b[39mtranscribe(file_path, num_speakers,model\u001b[39m=\u001b[39;49mmodel_medium)\n\u001b[0;32m     13\u001b[0m write_file(output, file_result_path)\n",
            "Cell \u001b[1;32mIn[4], line 50\u001b[0m, in \u001b[0;36mtranscribe\u001b[1;34m(path, num_speakers, model)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mif\u001b[39;00m duration \u001b[39m>\u001b[39m \u001b[39m4\u001b[39m \u001b[39m*\u001b[39m \u001b[39m60\u001b[39m \u001b[39m*\u001b[39m \u001b[39m60\u001b[39m:\n\u001b[0;32m     48\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAudio duration too long\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 50\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtranscribe(path)\n\u001b[0;32m     51\u001b[0m segments \u001b[39m=\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39msegments\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     53\u001b[0m num_speakers \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mmax\u001b[39m(\u001b[39mround\u001b[39m(num_speakers), \u001b[39m1\u001b[39m), \u001b[39mlen\u001b[39m(segments))\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\whisper\\transcribe.py:229\u001b[0m, in \u001b[0;36mtranscribe\u001b[1;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, **decode_options)\u001b[0m\n\u001b[0;32m    226\u001b[0m mel_segment \u001b[39m=\u001b[39m pad_or_trim(mel_segment, N_FRAMES)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mto(dtype)\n\u001b[0;32m    228\u001b[0m decode_options[\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m all_tokens[prompt_reset_since:]\n\u001b[1;32m--> 229\u001b[0m result: DecodingResult \u001b[39m=\u001b[39m decode_with_fallback(mel_segment)\n\u001b[0;32m    230\u001b[0m tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(result\u001b[39m.\u001b[39mtokens)\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m no_speech_threshold \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39m# no voice activity check\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\whisper\\transcribe.py:164\u001b[0m, in \u001b[0;36mtranscribe.<locals>.decode_with_fallback\u001b[1;34m(segment)\u001b[0m\n\u001b[0;32m    161\u001b[0m     kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mbest_of\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    163\u001b[0m options \u001b[39m=\u001b[39m DecodingOptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, temperature\u001b[39m=\u001b[39mt)\n\u001b[1;32m--> 164\u001b[0m decode_result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecode(segment, options)\n\u001b[0;32m    166\u001b[0m needs_fallback \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    168\u001b[0m     compression_ratio_threshold \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mand\u001b[39;00m decode_result\u001b[39m.\u001b[39mcompression_ratio \u001b[39m>\u001b[39m compression_ratio_threshold\n\u001b[0;32m    170\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\whisper\\decoding.py:811\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[0;32m    809\u001b[0m     options \u001b[39m=\u001b[39m replace(options, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 811\u001b[0m result \u001b[39m=\u001b[39m DecodingTask(model, options)\u001b[39m.\u001b[39;49mrun(mel)\n\u001b[0;32m    813\u001b[0m \u001b[39mreturn\u001b[39;00m result[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m single \u001b[39melse\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\whisper\\decoding.py:724\u001b[0m, in \u001b[0;36mDecodingTask.run\u001b[1;34m(self, mel)\u001b[0m\n\u001b[0;32m    721\u001b[0m tokens \u001b[39m=\u001b[39m tokens\u001b[39m.\u001b[39mrepeat_interleave(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_group, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(audio_features\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    723\u001b[0m \u001b[39m# call the main sampling loop\u001b[39;00m\n\u001b[1;32m--> 724\u001b[0m tokens, sum_logprobs, no_speech_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_main_loop(audio_features, tokens)\n\u001b[0;32m    726\u001b[0m \u001b[39m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[39;00m\n\u001b[0;32m    727\u001b[0m audio_features \u001b[39m=\u001b[39m audio_features[:: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_group]\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\whisper\\decoding.py:673\u001b[0m, in \u001b[0;36mDecodingTask._main_loop\u001b[1;34m(self, audio_features, tokens)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    672\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_len):\n\u001b[1;32m--> 673\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference\u001b[39m.\u001b[39;49mlogits(tokens, audio_features)\n\u001b[0;32m    675\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    676\u001b[0m             i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mno_speech \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    677\u001b[0m         ):  \u001b[39m# save no_speech_probs\u001b[39;00m\n\u001b[0;32m    678\u001b[0m             probs_at_sot \u001b[39m=\u001b[39m logits[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msot_index]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\whisper\\decoding.py:157\u001b[0m, in \u001b[0;36mPyTorchInference.logits\u001b[1;34m(self, tokens, audio_features)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m tokens\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_token_length:\n\u001b[0;32m    154\u001b[0m     \u001b[39m# only need to use the last token except in the first forward pass\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     tokens \u001b[39m=\u001b[39m tokens[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecoder(tokens, audio_features, kv_cache\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkv_cache)\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\whisper\\model.py:211\u001b[0m, in \u001b[0;36mTextDecoder.forward\u001b[1;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[0;32m    208\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(xa\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    210\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[1;32m--> 211\u001b[0m     x \u001b[39m=\u001b[39m block(x, xa, mask\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmask, kv_cache\u001b[39m=\u001b[39;49mkv_cache)\n\u001b[0;32m    213\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln(x)\n\u001b[0;32m    214\u001b[0m logits \u001b[39m=\u001b[39m (\n\u001b[0;32m    215\u001b[0m     x \u001b[39m@\u001b[39m torch\u001b[39m.\u001b[39mtranspose(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_embedding\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdtype), \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    216\u001b[0m )\u001b[39m.\u001b[39mfloat()\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\whisper\\model.py:138\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[1;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[0;32m    136\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_ln(x), mask\u001b[39m=\u001b[39mmask, kv_cache\u001b[39m=\u001b[39mkv_cache)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_attn:\n\u001b[1;32m--> 138\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcross_attn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcross_attn_ln(x), xa, kv_cache\u001b[39m=\u001b[39;49mkv_cache)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    139\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_ln(x))\n\u001b[0;32m    140\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\whisper\\model.py:90\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[0;32m     87\u001b[0m     k \u001b[39m=\u001b[39m kv_cache[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey]\n\u001b[0;32m     88\u001b[0m     v \u001b[39m=\u001b[39m kv_cache[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue]\n\u001b[1;32m---> 90\u001b[0m wv, qk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqkv_attention(q, k, v, mask)\n\u001b[0;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(wv), qk\n",
            "File \u001b[1;32mc:\\Users\\vtnguye2503\\Projects\\teli-ai\\env\\lib\\site-packages\\whisper\\model.py:108\u001b[0m, in \u001b[0;36mMultiHeadAttention.qkv_attention\u001b[1;34m(self, q, k, v, mask)\u001b[0m\n\u001b[0;32m    105\u001b[0m qk \u001b[39m=\u001b[39m qk\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m    107\u001b[0m w \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(qk, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(q\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m--> 108\u001b[0m \u001b[39mreturn\u001b[39;00m (w \u001b[39m@\u001b[39;49m v)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m), qk\u001b[39m.\u001b[39mdetach()\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "classification = '/not_interested'#Change this for different type of classification\n",
        "audio_path = './asset/testing_audio' + classification\n",
        "result_path = './asset/testing_result' + classification\n",
        "\n",
        "\n",
        "mp3_to_wav(audio_path)\n",
        "output=''\n",
        "for file_name in os.listdir(audio_path):\n",
        "    if os.path.isfile(os.path.join(audio_path, file_name)):\n",
        "        file_path = os.path.join(audio_path, file_name)\n",
        "        file_result_path = os.path.join(result_path, file_name[:-4] + '.txt')\n",
        "        output =transcribe(file_path, num_speakers,model=model_medium)\n",
        "        write_file(output, file_result_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8yf8Oow4hgUU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
